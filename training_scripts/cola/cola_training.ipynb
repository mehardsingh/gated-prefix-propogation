{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOLNMetrIJ7E9hCUA8SXaTE"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install datasets\n","!pip install transformers\n","!pip install evaluate\n","!pip install accelerate -U\n","!pip install numpy"],"metadata":{"id":"PBCGm_tbctx2"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C6IJfdy7cK3o"},"outputs":[],"source":["from trainer_compatible.bert_prefix import BertForSequenceClassification_Prefix\n","from trainer_compatible.bert_prefix_gated import BertForSequenceClassification_Prefix_Gated\n","from trainer_compatible.roberta_prefix import RobertaForSequenceClassification_Prefix\n","from trainer_compatible.roberta_prefix_gated import RobertaForSequenceClassification_Prefix_Gated"]},{"cell_type":"code","source":["from datasets import load_dataset\n","from transformers import AutoTokenizer, AutoConfig\n","from transformers import DataCollatorWithPadding\n","from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n","import evaluate\n","import numpy as np\n","import random"],"metadata":{"id":"lBXp9VivcfgG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset = load_dataset(\"glue\", \"cola\")\n","train_ds = dataset[\"train\"]\n","test_ds = dataset[\"validation\"]\n","accuracy = evaluate.load(\"accuracy\")"],"metadata":{"id":"qGNoj7tccheI","executionInfo":{"status":"ok","timestamp":1714253091083,"user_tz":240,"elapsed":145,"user":{"displayName":"Padhma Muniraj","userId":"05530362682888749155"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["def compute_metrics(eval_pred):\n","  predictions, labels = eval_pred\n","  predictions = np.argmax(predictions, axis=1)\n","  return accuracy.compute(predictions=predictions, references=labels)"],"metadata":{"id":"XNBs8Gb7ck9g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prefix_lengths = [1, 4, 8, 16, 64]\n","results = {}\n","num_labels=2"],"metadata":{"id":"rAW47hv2colj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# BERT Architecture Models"],"metadata":{"id":"bZU1gUZecz0z"}},{"cell_type":"code","source":["model_name = \"google-bert/bert-base-uncased\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)"],"metadata":{"id":"0fdBjbu7dIR6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def preprocess_function(sample):\n","  return tokenizer(sample[\"sentence\"], truncation=True, max_length=256)\n","\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n","tokenized_train_ds = train_ds.map(preprocess_function, batched=True)\n","tokenized_test_ds = test_ds.map(preprocess_function, batched=True)"],"metadata":{"id":"kiX3UkwtdI6X"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Full Parameterized BERT"],"metadata":{"id":"0g08jjztc4n3"}},{"cell_type":"code","source":["model = AutoModelForSequenceClassification.from_pretrained(model_name)"],"metadata":{"id":"Q4PESTtJdP5J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for param in model.parameters():\n","    param.requires_grad = True"],"metadata":{"id":"eTeFYS1XczkK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["training_args = TrainingArguments(\n","    output_dir=f\"./results/cola/bert\",\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=16,\n","    per_device_eval_batch_size=16,\n","    num_train_epochs=10,\n","    weight_decay=0.01,\n","    evaluation_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    logging_strategy=\"epoch\",\n","    save_total_limit=1,\n","    load_best_model_at_end=True\n",")\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_train_ds,\n","    eval_dataset=tokenized_test_ds,\n","    tokenizer=tokenizer,\n","    data_collator=data_collator,\n","    compute_metrics=compute_metrics,\n",")\n","\n","trainer.train()"],"metadata":{"id":"dz3x7_RQdNdM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["eval_result = trainer.evaluate()\n","print(eval_result)"],"metadata":{"id":"phtu3i1odVpY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### BERT Prefix"],"metadata":{"id":"xdhEoZFcc8RM"}},{"cell_type":"code","source":["for prefix_len in prefix_lengths:\n","  print(f\"Training with prefix length: {prefix_len} for BERT Prefix.\")\n","  config = AutoConfig.from_pretrained(model_name)\n","  config.num_labels = num_labels\n","  config.prefix_len = prefix_len\n","  model = BertForSequenceClassification_Prefix.from_pretrained(model_name,\n","                                                               config=config)\n","  for name, param in model.named_parameters():\n","    if not name in [\n","        \"bert.encoder.prefix\", \"bert.pooler.dense.weight\",\n","        \"bert.pooler.dense.bias\", \"classifier.weight\", \"classifier.bias\"\n","    ]:\n","      param.requires_grad = False\n","  total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","  print(f\"Number of parameters: {total_params}\")"],"metadata":{"id":"Qaki04rKczhy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["training_args = TrainingArguments(\n","    output_dir=f\"./results/cola/bert_prefix_{prefix_len}\",\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=16,\n","    per_device_eval_batch_size=16,\n","    num_train_epochs=50,\n","    weight_decay=0.01,\n","    evaluation_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    logging_strategy=\"epoch\",\n","    save_total_limit=1,\n","    load_best_model_at_end=True)\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_train_ds,\n","    eval_dataset=tokenized_test_ds,\n","    tokenizer=tokenizer,\n","    data_collator=data_collator,\n","    compute_metrics=compute_metrics,\n",")"],"metadata":{"id":"eustnAslePai"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["eval_result = trainer.evaluate()\n","print(eval_result)"],"metadata":{"id":"ZNjmvlGHeYtR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### BERT Gated Prefix"],"metadata":{"id":"r5sFeTctdCUs"}},{"cell_type":"code","source":["for prefix_len in prefix_lengths:\n","  print(f\"Training with prefix length: {prefix_len} for BERT Prefix Gated.\")\n","  config = AutoConfig.from_pretrained(model_name)\n","  config.num_labels = num_labels\n","  config.prefix_len = prefix_len\n","  model = BertForSequenceClassification_Prefix_Gated.from_pretrained(\n","      model_name, config=config)\n","\n","  for name, param in model.named_parameters():\n","    if not (name in [\n","        \"bert.encoder.prefix\", \"bert.pooler.dense.weight\",\n","        \"bert.pooler.dense.bias\", \"classifier.weight\", \"classifier.bias\"\n","    ]) and not (\"gate_mlps\" in name):\n","      param.requires_grad = False\n","\n","  total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","  print(f\"Number of parameters: {total_params}\")"],"metadata":{"id":"R2NqP-ZVczfe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["training_args = TrainingArguments(\n","    output_dir=f\"./results/multinli/bert_prefix_gated_{prefix_len}\",\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=16,\n","    per_device_eval_batch_size=16,\n","    num_train_epochs=30,\n","    weight_decay=0.01,\n","    evaluation_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    logging_strategy=\"epoch\",\n","    save_total_limit=1,\n","    load_best_model_at_end=True)\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_train_ds,\n","    eval_dataset=tokenized_test_ds,\n","    tokenizer=tokenizer,\n","    data_collator=data_collator,\n","    compute_metrics=compute_metrics,\n",")"],"metadata":{"id":"ByPkosXWczUy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["eval_result = trainer.evaluate()\n","print(eval_result)"],"metadata":{"id":"fdgd-Zxae3tn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# RoBERTa Architecture Models"],"metadata":{"id":"gVAYuuIgc1-F"}},{"cell_type":"markdown","source":["#### Full Parameterized RoBERTa"],"metadata":{"id":"we2ZDPirdmnN"}},{"cell_type":"code","source":["model_name = \"FacebookAI/roberta-base\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)"],"metadata":{"id":"iWwUh9R_c4Ab"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def preprocess_function(sample):\n","  return tokenizer(sample[\"sentence\"], truncation=True, max_length=256)"],"metadata":{"id":"OYWRGI2fet48"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def preprocess_function(sample):\n","  return tokenizer(sample[\"sentence\"], truncation=True, max_length=256)\n","\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n","tokenized_train_ds = train_ds.map(preprocess_function, batched=True)\n","tokenized_test_ds = test_ds.map(preprocess_function, batched=True)\n","\n","model = AutoModelForSequenceClassification.from_pretrained(model_name)"],"metadata":{"id":"IjoAlA9_evkg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for param in model.parameters():\n","    param.requires_grad = True"],"metadata":{"id":"dQ50jwsbezxw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","print(f\"Number of parameters: {total_params}\")"],"metadata":{"id":"ldiCNGGle1L4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["training_args = TrainingArguments(\n","    output_dir=f\"./results/cola/roberta\",\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=16,\n","    per_device_eval_batch_size=16,\n","    num_train_epochs=10,\n","    weight_decay=0.01,\n","    evaluation_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    logging_strategy=\"epoch\",\n","    save_total_limit=1,\n","    load_best_model_at_end=True\n",")\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_train_ds,\n","    eval_dataset=tokenized_test_ds,\n","    tokenizer=tokenizer,\n","    data_collator=data_collator,\n","    compute_metrics=compute_metrics,\n",")\n","\n","trainer.train()"],"metadata":{"id":"RbriL8p-e1k3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["eval_result = trainer.evaluate()\n","print(eval_result)"],"metadata":{"id":"WIP3zN-9e-bT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### RoBERTa Prefix"],"metadata":{"id":"rkWIlwbydr9i"}},{"cell_type":"code","source":["for prefix_len in prefix_lengths:\n","  print(f\"Training with prefix length: {prefix_len} for RoBERTa Prefix.\")\n","  config = AutoConfig.from_pretrained(model_name)\n","  config.num_labels = num_labels\n","  config.prefix_len = prefix_len\n","  model = RobertaForSequenceClassification_Prefix.from_pretrained(\n","      model_name, config=config)\n","\n","  for name, param in model.named_parameters():\n","    if not name in [\n","        \"roberta.encoder.prefix\", \"roberta.pooler.dense.weight\",\n","        \"roberta.pooler.dense.bias\", \"classifier.dense.weight\",\n","        \"classifier.dense.bias\", \"classifier.out_proj.weight\",\n","        \"classifier.out_proj.bias\"\n","    ]:\n","      param.requires_grad = False\n","\n","  total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","  print(f\"Number of parameters: {total_params}\")"],"metadata":{"id":"I3RBVCYwducx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["training_args = TrainingArguments(\n","      output_dir=f\"./results/cola/roberta_prefix_{prefix_len}\",\n","      learning_rate=2e-5,\n","      per_device_train_batch_size=16,\n","      per_device_eval_batch_size=16,\n","      num_train_epochs=30,\n","      weight_decay=0.01,\n","      evaluation_strategy=\"epoch\",\n","      save_strategy=\"epoch\",\n","      logging_strategy=\"epoch\",\n","      save_total_limit=1,\n","      load_best_model_at_end=True)\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_train_ds,\n","    eval_dataset=tokenized_test_ds,\n","    tokenizer=tokenizer,\n","    data_collator=data_collator,\n","    compute_metrics=compute_metrics,\n",")"],"metadata":{"id":"O1XgTxyTfHyM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["eval_result = trainer.evaluate()\n","print(eval_result)"],"metadata":{"id":"vBgZeKJ_e-3e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### RoBERTa Gated Prefix"],"metadata":{"id":"KBTmoFQ5dvQu"}},{"cell_type":"code","source":["for prefix_len in prefix_lengths:\n","  print(f\"Training with prefix length: {prefix_len} for RoBERTa Prefix Gated.\")\n","  config = AutoConfig.from_pretrained(model_name)\n","  config.num_labels = num_labels\n","  config.prefix_len = prefix_len\n","  model = RobertaForSequenceClassification_Prefix_Gated.from_pretrained(\n","      model_name, config=config)\n","  for name, param in model.named_parameters():\n","    if not (name in [\n","        \"roberta.encoder.prefix\", \"roberta.pooler.dense.weight\",\n","        \"roberta.pooler.dense.bias\", \"classifier.dense.weight\",\n","        \"classifier.dense.bias\", \"classifier.out_proj.weight\",\n","        \"classifier.out_proj.bias\"\n","    ]) and not (\"gate_mlps\" in name):\n","      param.requires_grad = False\n","\n","  total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","  print(f\"Number of parameters: {total_params}\")"],"metadata":{"id":"WjRHFoLQdxcD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["training_args = TrainingArguments(\n","      output_dir=f\"./results/cola/roberta_prefix_gated_{prefix_len}\",\n","      learning_rate=2e-5,\n","      per_device_train_batch_size=16,\n","      per_device_eval_batch_size=16,\n","      num_train_epochs=30,\n","      weight_decay=0.01,\n","      evaluation_strategy=\"epoch\",\n","      save_strategy=\"epoch\",\n","      logging_strategy=\"epoch\",\n","      save_total_limit=1,\n","      load_best_model_at_end=True)\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_train_ds,\n","    eval_dataset=tokenized_test_ds,\n","    tokenizer=tokenizer,\n","    data_collator=data_collator,\n","    compute_metrics=compute_metrics,\n",")"],"metadata":{"id":"mYIfLeF9fRAj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["eval_result = trainer.evaluate()\n","print(eval_result)"],"metadata":{"id":"-RDwQD46e_Md"},"execution_count":null,"outputs":[]}]}